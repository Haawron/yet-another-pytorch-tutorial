{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mln.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN5+z8VLyue5D0J2PVWb56Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"eR2iS_cBcQ1m"},"source":["# Mixture Logits Network (MLN)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rNN6G5xWcW-x","executionInfo":{"status":"ok","timestamp":1610155710721,"user_tz":-540,"elapsed":4300,"user":{"displayName":"Sungjoon Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64","userId":"10728677910935649939"}},"outputId":"f8d79d63-e64e-40a2-b05e-3d2b3d60587e"},"source":["import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.distributions as TD\n","from torch.autograd import Variable\n","from collections import OrderedDict\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","np.set_printoptions(precision=2)\n","torch.set_printoptions(precision=2)\n","print (\"PyTorch version:[%s].\"%(torch.__version__))\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print (\"device:[%s].\"%(device))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["PyTorch version:[1.7.0+cu101].\n","device:[cuda:0].\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bdSeOD6fdmKy"},"source":["### Define the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cvgVHp9hcXJK","executionInfo":{"status":"ok","timestamp":1610155790943,"user_tz":-540,"elapsed":710,"user":{"displayName":"Sungjoon Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64","userId":"10728677910935649939"}},"outputId":"945b7636-9b9e-4808-d4ca-533c41317682"},"source":["def np2tc(x_np): return torch.from_numpy(x_np).float().to(device)\n","def tc2np(x_tc): return x_tc.detach().cpu().numpy()\n","\n","class MixtureOfLogits(nn.Module):\n","    def __init__(self,\n","                 in_dim     = 64,   # input feature dimension \n","                 y_dim      = 10,   # number of classes \n","                 k          = 5,    # number of mixtures\n","                 sig_min    = 1e-4, # minimum sigma\n","                 sig_max    = None, # maximum sigma\n","                 SHARE_SIG  = True  # share sigma among mixture\n","                 ):\n","        super(MixtureOfLogits,self).__init__()\n","        self.in_dim     = in_dim    # Q\n","        self.y_dim      = y_dim     # D\n","        self.k          = k         # K\n","        self.sig_min    = sig_min\n","        self.sig_max    = sig_max\n","        self.SHARE_SIG  = SHARE_SIG\n","        self.build_graph()\n","\n","    def build_graph(self):\n","        self.fc_pi      = nn.Linear(self.in_dim,self.k)\n","        self.fc_mu      = nn.Linear(self.in_dim,self.k*self.y_dim)\n","        if self.SHARE_SIG:\n","            self.fc_sigma   = nn.Linear(self.in_dim,self.k)\n","        else:\n","            self.fc_sigma   = nn.Linear(self.in_dim,self.k*self.y_dim)\n","\n","    def forward(self,x):\n","        \"\"\"\n","            :param x: [N x Q]\n","        \"\"\"\n","        pi_logit        = self.fc_pi(x)                                 # [N x K]\n","        pi              = torch.softmax(pi_logit,dim=1)                 # [N x K]\n","        mu              = self.fc_mu(x)                                 # [N x KD]\n","        mu              = torch.reshape(mu,(-1,self.k,self.y_dim))      # [N x K x D]\n","        if self.SHARE_SIG:\n","            sigma       = self.fc_sigma(x)                              # [N x K]\n","            sigma       = sigma.unsqueeze(dim=-1)                       # [N x K x 1]\n","            sigma       = sigma.expand_as(mu)                           # [N x K x D]\n","        else:\n","            sigma       = self.fc_sigma(x)                              # [N x KD]\n","        sigma           = torch.reshape(sigma,(-1,self.k,self.y_dim))   # [N x K x D]\n","        if self.sig_max is None:\n","            sigma = self.sig_min + torch.exp(sigma)                     # [N x K x D]\n","        else:\n","            sig_range = (self.sig_max-self.sig_min)\n","            sigma = self.sig_min + sig_range*torch.sigmoid(sigma)       # [N x K x D]\n","        mol_out = {'pi':pi,'mu':mu,'sigma':sigma}\n","        return mol_out\n","\n","class MixtureLogitNetwork(nn.Module):\n","    def __init__(self,\n","                 name       = 'mln',        # name\n","                 x_dim      = [1,28,28],    # input dimension\n","                 k_size     = 3,            # kernel size\n","                 c_dims     = [32,64],      # conv channel dimensions\n","                 p_sizes    = [2,2],        # pooling sizes\n","                 h_dims     = [128],        # hidden dimensions\n","                 y_dim      = 10,           # output dimension\n","                 USE_BN     = True,         # whether to use batch-norm\n","                 k          = 5,            # number of mixtures\n","                 sig_min    = 1e-4,         # minimum sigma\n","                 sig_max    = 10,           # maximum sigma\n","                 mu_min     = -3,           # minimum mu (init)\n","                 mu_max     = +3,           # maximum mu (init)\n","                 SHARE_SIG  = True          \n","                 ):\n","        super(MixtureLogitNetwork,self).__init__()\n","        self.name       = name\n","        self.x_dim      = x_dim\n","        self.k_size     = k_size\n","        self.c_dims     = c_dims\n","        self.p_sizes    = p_sizes\n","        self.h_dims     = h_dims\n","        self.y_dim      = y_dim\n","        self.USE_BN     = USE_BN\n","        self.k          = k\n","        self.sig_min    = sig_min\n","        self.sig_max    = sig_max\n","        self.mu_min     = mu_min\n","        self.mu_max     = mu_max\n","        self.SHARE_SIG  = SHARE_SIG\n","        self.build_graph()\n","        self.init_param()\n","\n","    def build_graph(self):\n","        self.layers = []\n","        # Conv layers\n","        prev_c_dim = self.x_dim[0] # input channel \n","        for (c_dim,p_size) in zip(self.c_dims,self.p_sizes):\n","            self.layers.append(\n","                nn.Conv2d(\n","                    in_channels  = prev_c_dim,\n","                    out_channels = c_dim,\n","                    kernel_size  = self.k_size,\n","                    stride       = (1,1),\n","                    padding      = self.k_size//2\n","                    ) # conv\n","                )\n","            if self.USE_BN:\n","                self.layers.append(\n","                    nn.BatchNorm2d(num_features=c_dim)\n","                )\n","            self.layers.append(nn.ReLU())\n","            self.layers.append(\n","                nn.MaxPool2d(kernel_size=(p_size,p_size),stride=(p_size,p_size))\n","                )\n","            # self.layers.append(nn.Dropout2d(p=0.1))  # p: to be zero-ed\n","            prev_c_dim = c_dim \n","        # Dense layers\n","        self.layers.append(nn.Flatten())\n","        p_prod = np.prod(self.p_sizes)\n","        prev_h_dim = prev_c_dim*(self.x_dim[1]//p_prod)*(self.x_dim[2]//p_prod)\n","        for h_dim in self.h_dims:\n","            self.layers.append(\n","                nn.Linear(\n","                    in_features  = prev_h_dim,\n","                    out_features = h_dim,\n","                    bias         = True\n","                    )\n","                )\n","            self.layers.append(nn.ReLU(True))  # activation\n","            self.layers.append(nn.Dropout2d(p=0.1))  # p: to be zero-ed\n","            prev_h_dim = h_dim\n","        # Final mixture of logits layer\n","        mol = MixtureOfLogits(\n","            in_dim      = prev_h_dim,  \n","            y_dim       = self.y_dim, \n","            k           = self.k,\n","            sig_min     = self.sig_min,\n","            sig_max     = self.sig_max,\n","            SHARE_SIG   = self.SHARE_SIG\n","        )\n","        self.layers.append(mol)\n","        # Concatanate all layers\n","        self.net = nn.Sequential()\n","        for l_idx,layer in enumerate(self.layers):\n","            layer_name = \"%s_%02d\"%(type(layer).__name__.lower(),l_idx)\n","            self.net.add_module(layer_name,layer)\n","\n","    def forward(self,x):\n","        mln_out = self.net(x)\n","        return mln_out # mu:[N x K x D] / pi:[N x K] / sigma:[N x K x D]\n","\n","    def init_param(self):\n","        for m in self.modules():\n","            if isinstance(m,nn.Conv2d): # init conv\n","                nn.init.kaiming_normal_(m.weight)\n","                nn.init.zeros_(m.bias)\n","            if isinstance(m,nn.Linear): # lnit dense\n","                nn.init.kaiming_normal_(m.weight)\n","                nn.init.zeros_(m.bias)\n","        # Heuristic: fc_mu.bias ~ Uniform(mu_min,mu_max)\n","        self.layers[-1].fc_mu.bias.data.uniform_(self.mu_min,self.mu_max)\n","\n","def mln_uncertainties(pi,mu,sigma):\n","    \"\"\"\n","        :param pi:      [N x K]\n","        :param mu:      [N x K x D]\n","        :param sigma:   [N x K x D]\n","    \"\"\"\n","    \n","\n","\n","def mace_loss(pi,mu,sigma,target,alea_weight=1.0):\n","    \"\"\"\n","        :param pi:      [N x K]\n","        :param mu:      [N x K x D]\n","        :param sigma:   [N x K x D]\n","        :param target:  [N x D]\n","    \"\"\"\n","    # $\\mu$\n","    mu_hat = torch.softmax(mu,dim=2) # logit to prob [N x K x D]\n","    log_mu_hat = torch.log(mu_hat+1e-6) # [N x K x D]\n","    # $\\pi$\n","    pi_usq = torch.unsqueeze(pi,2) # [N x K x 1]\n","    pi_exp = pi_usq.expand_as(mu) # [N x K x D]\n","    # target\n","    target_usq =  torch.unsqueeze(target,1) # [N x 1 x D]\n","    target_exp =  target_usq.expand_as(mu) # [N x K x D]\n","    # CE loss\n","    ce_exp = -target_exp*log_mu_hat # CE [N x K x D]\n","    ace_exp = ce_loss_exp / sigma # attenuated CE [N x K x D]\n","    mace_exp = torch.mul(pi_exp,ace_exp) # mixtured attenuated CE [N x K x D]\n","    mace = torch.sum(mace_exp,dim=1) # [N x D]\n","    mace = torch.sum(mace,dim=1) # [N]\n","    mace_avg = torch.mean(mace) # [1]\n","    # \n","\n","    # Accumulate outputs\n","    loss_out = {'mace':mace,'mace_avg':mace_avg}\n","    return loss_out\n","\n","\n","# Demo forward path of MLN\n","M = MixtureLogitNetwork(k=3,SHARE_SIG=True).to(device)\n","x = torch.rand([2]+M.x_dim).to(device)\n","mln_out = M.forward(x)\n","pi,mu,sigma = mln_out['pi'],mln_out['mu'],mln_out['sigma']\n","print (\"x:       %s\"%(tc2np(x).shape,))\n","print (\"\\n=>\")\n","print (\"\\npi:    %s\\n%s\"%(tc2np(pi).shape,tc2np(pi)))\n","print (\"\\nmu:    %s\\n%s\"%(tc2np(mu).shape,tc2np(mu)))\n","print (\"\\nsigma: %s\\n%s\"%(tc2np(sigma).shape,tc2np(sigma)))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["x:       (2, 1, 28, 28)\n","\n","=>\n","\n","pi:    (2, 3)\n","[[0.11 0.01 0.88]\n"," [0.04 0.   0.96]]\n","\n","mu:    (2, 3, 10)\n","[[[ 0.26  2.32 -3.59  1.17 -1.26  2.32  3.06  3.34 -3.51 -1.03]\n","  [-0.15 -1.64  2.3   1.65 -1.11  3.38  2.89 -0.12 -0.91 -0.32]\n","  [-0.94  0.69 -2.35  1.37  1.01 -0.01 -1.42 -2.29 -0.1   0.56]]\n","\n"," [[ 0.3   1.75 -5.37  1.78 -1.82  1.37  2.4   3.38 -2.75  0.54]\n","  [ 2.34 -1.03  2.93  1.96 -1.24  3.06  2.88 -0.17 -1.56 -0.13]\n","  [-1.88 -0.66 -2.    0.59  0.94 -1.7  -1.88 -1.69  1.5  -0.35]]]\n","\n","sigma: (2, 3, 10)\n","[[[1.91 1.91 1.91 1.91 1.91 1.91 1.91 1.91 1.91 1.91]\n","  [1.96 1.96 1.96 1.96 1.96 1.96 1.96 1.96 1.96 1.96]\n","  [1.09 1.09 1.09 1.09 1.09 1.09 1.09 1.09 1.09 1.09]]\n","\n"," [[2.36 2.36 2.36 2.36 2.36 2.36 2.36 2.36 2.36 2.36]\n","  [3.44 3.44 3.44 3.44 3.44 3.44 3.44 3.44 3.44 3.44]\n","  [1.1  1.1  1.1  1.1  1.1  1.1  1.1  1.1  1.1  1.1 ]]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gjb0EpA0cXWq","executionInfo":{"status":"aborted","timestamp":1610121811285,"user_tz":-540,"elapsed":1391,"user":{"displayName":"Sungjoon Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64","userId":"10728677910935649939"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"znG-pOLgcXah","executionInfo":{"status":"aborted","timestamp":1610121811286,"user_tz":-540,"elapsed":1390,"user":{"displayName":"Sungjoon Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64","userId":"10728677910935649939"}}},"source":[""],"execution_count":null,"outputs":[]}]}