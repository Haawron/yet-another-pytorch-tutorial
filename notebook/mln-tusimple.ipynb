{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wVPrFTFw2tY"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/sjchoi86/yet-another-pytorch-tutorial/blob/main/notebook/mln.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/sjchoi86/yet-another-pytorch-tutorial/blob/main/notebook/mln.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View Source</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eR2iS_cBcQ1m"
   },
   "source": [
    "# Mixture Logits Network (MLN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1105,
     "status": "ok",
     "timestamp": 1610363294248,
     "user": {
      "displayName": "Sungjoon Choi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64",
      "userId": "10728677910935649939"
     },
     "user_tz": -540
    },
    "id": "rNN6G5xWcW-x",
    "outputId": "da76efff-da43-410b-8cd5-cb71ed850d51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.5.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "import math, random\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as TD\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets,transforms\n",
    "from collections import OrderedDict\n",
    "from PIL import Image, ImageDraw\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "np.set_printoptions(precision=2)\n",
    "torch.set_printoptions(precision=2)\n",
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdSeOD6fdmKy"
   },
   "source": [
    "## Define the model\n",
    "- Mixture of Logits (MoL)\n",
    "- Mixture Logits Network (MLN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4471,
     "status": "ok",
     "timestamp": 1610363297634,
     "user": {
      "displayName": "Sungjoon Choi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64",
      "userId": "10728677910935649939"
     },
     "user_tz": -540
    },
    "id": "cvgVHp9hcXJK",
    "outputId": "258e7a6c-eb49-495f-a5b8-66725af03bff"
   },
   "outputs": [],
   "source": [
    "def np2tc(x_np): return torch.from_numpy(x_np).float().to(device)\n",
    "def tc2np(x_tc): return x_tc.detach().cpu().numpy()\n",
    "Normalizer = [\n",
    "    nn.BatchNorm2d,\n",
    "    nn.InstanceNorm2d,\n",
    "][1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture of Logits (MoL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4471,
     "status": "ok",
     "timestamp": 1610363297634,
     "user": {
      "displayName": "Sungjoon Choi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64",
      "userId": "10728677910935649939"
     },
     "user_tz": -540
    },
    "id": "cvgVHp9hcXJK",
    "outputId": "258e7a6c-eb49-495f-a5b8-66725af03bff"
   },
   "outputs": [],
   "source": [
    "def summarize(name, blocks, x):\n",
    "    print('='*60)\n",
    "    print(f'Module [{name}]')\n",
    "    print(f'\\tInput{str(list(x.shape)):>47}')\n",
    "    with torch.no_grad():\n",
    "        count_layer = 1\n",
    "        for i, block in enumerate(blocks):\n",
    "            x = block(x)\n",
    "            print(f'\\tBlock #{i+1}')\n",
    "            for layer in block:\n",
    "                class_name = str(layer.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "                print(f'\\t  Layer {f\"#{count_layer}\":>3} {class_name}')\n",
    "                count_layer += 1\n",
    "            print(f'{str(list(x.shape)):>60}')\n",
    "            \n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = None\n",
    "        self.blocks = None\n",
    "    def build_graph(self, blocks):\n",
    "        return nn.ModuleList(map(lambda block: nn.Sequential(*block), blocks))\n",
    "    def __validate(self):\n",
    "        assert self.name,   '`self.name` is not defined'\n",
    "        assert self.blocks, '`self.blocks` is not defined. Build one using self.build_graph(blocks)'\n",
    "    def summarize(self, x):\n",
    "        self.__validate()\n",
    "        summarize(self.name, self.blocks, x) \n",
    "    def forward(self, x):\n",
    "        self.__validate()\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "class Extracter(BaseModel):\n",
    "    def __init__(self, name='Feature Extracter'):\n",
    "        super().__init__() \n",
    "        self.name = name\n",
    "        blocks = [\n",
    "            (\n",
    "                nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=True),\n",
    "                Normalizer(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ), (\n",
    "                nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=True),\n",
    "                Normalizer(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        ]\n",
    "        self.blocks = self.build_graph(blocks)\n",
    "        \n",
    "class ModelMuSigma(BaseModel):\n",
    "    def __init__(self, dim_k, name='MoL MuSigma'):\n",
    "        super().__init__() \n",
    "        self.name = name\n",
    "        blocks = [\n",
    "            (\n",
    "                nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True),\n",
    "                Normalizer(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ), (    \n",
    "                nn.ConvTranspose2d(64, dim_k, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True),\n",
    "            ),\n",
    "        ]\n",
    "        self.blocks = self.build_graph(blocks) \n",
    "\n",
    "class ModelPi(BaseModel):\n",
    "    def __init__(self, dim_k, name='MoL Pi'):\n",
    "        super().__init__() \n",
    "        self.name = name\n",
    "        blocks = [\n",
    "            (\n",
    "                nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=True),\n",
    "                Normalizer(256),\n",
    "                nn.LeakyReLU(.2, inplace=True),\n",
    "            ), (    \n",
    "                nn.Conv2d(256, 256, kernel_size=4, stride=2, padding=1, bias=True),\n",
    "                Normalizer(256),\n",
    "                nn.LeakyReLU(.2, inplace=True),\n",
    "            ), (\n",
    "                nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=True),\n",
    "                Normalizer(512),\n",
    "                nn.LeakyReLU(.2, inplace=True),\n",
    "            ), (\n",
    "                nn.Conv2d(512, dim_k//2, kernel_size=5, stride=5, padding=0, bias=True),\n",
    "            ), (\n",
    "                nn.Flatten(),\n",
    "                nn.Softmax(1),\n",
    "            )\n",
    "        ]\n",
    "        self.blocks = self.build_graph(blocks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture Logits Network (MLN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4471,
     "status": "ok",
     "timestamp": 1610363297634,
     "user": {
      "displayName": "Sungjoon Choi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64",
      "userId": "10728677910935649939"
     },
     "user_tz": -540
    },
    "id": "cvgVHp9hcXJK",
    "outputId": "258e7a6c-eb49-495f-a5b8-66725af03bff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Module [Feature Extracter]\n",
      "\tInput                               [4, 3, 180, 320]\n",
      "\tBlock #1\n",
      "\t  Layer  #1 Conv2d\n",
      "\t  Layer  #2 InstanceNorm2d\n",
      "\t  Layer  #3 ReLU\n",
      "                                            [4, 64, 90, 160]\n",
      "\tBlock #2\n",
      "\t  Layer  #4 Conv2d\n",
      "\t  Layer  #5 InstanceNorm2d\n",
      "\t  Layer  #6 ReLU\n",
      "                                            [4, 128, 45, 80]\n",
      "============================================================\n",
      "Module [MoL Pi]\n",
      "\tInput                               [4, 128, 45, 80]\n",
      "\tBlock #1\n",
      "\t  Layer  #1 Conv2d\n",
      "\t  Layer  #2 InstanceNorm2d\n",
      "\t  Layer  #3 LeakyReLU\n",
      "                                            [4, 256, 22, 40]\n",
      "\tBlock #2\n",
      "\t  Layer  #4 Conv2d\n",
      "\t  Layer  #5 InstanceNorm2d\n",
      "\t  Layer  #6 LeakyReLU\n",
      "                                            [4, 256, 11, 20]\n",
      "\tBlock #3\n",
      "\t  Layer  #7 Conv2d\n",
      "\t  Layer  #8 InstanceNorm2d\n",
      "\t  Layer  #9 LeakyReLU\n",
      "                                             [4, 512, 5, 10]\n",
      "\tBlock #4\n",
      "\t  Layer #10 Conv2d\n",
      "                                                [4, 5, 1, 2]\n",
      "\tBlock #5\n",
      "\t  Layer #11 Flatten\n",
      "\t  Layer #12 Softmax\n",
      "                                                     [4, 10]\n",
      "============================================================\n",
      "Module [MoL Mu]\n",
      "\tInput                               [4, 128, 45, 80]\n",
      "\tBlock #1\n",
      "\t  Layer  #1 ConvTranspose2d\n",
      "\t  Layer  #2 InstanceNorm2d\n",
      "\t  Layer  #3 ReLU\n",
      "                                            [4, 64, 90, 160]\n",
      "\tBlock #2\n",
      "\t  Layer  #4 ConvTranspose2d\n",
      "                                           [4, 10, 180, 320]\n",
      "============================================================\n",
      "Module [MoL Sigma]\n",
      "\tInput                               [4, 128, 45, 80]\n",
      "\tBlock #1\n",
      "\t  Layer  #1 ConvTranspose2d\n",
      "\t  Layer  #2 InstanceNorm2d\n",
      "\t  Layer  #3 ReLU\n",
      "                                            [4, 64, 90, 160]\n",
      "\tBlock #2\n",
      "\t  Layer  #4 ConvTranspose2d\n",
      "                                           [4, 10, 180, 320]\n"
     ]
    }
   ],
   "source": [
    "class MixtureLogitsNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim_k      = 10,\n",
    "                 # todo: multi class를 위한 dim_y 추가, SHARE_SIG도 추가\n",
    "                 sig_min    = 1e-4, \n",
    "                 mu_min     = -3,           # minimum mu (init)\n",
    "                 mu_max     = +3,           # maximum mu (init)\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.dim_k   = dim_k\n",
    "        self.sig_min = sig_min\n",
    "        self.mu_min  = mu_min\n",
    "        self.mu_max  = mu_max\n",
    "        self.build_graph()\n",
    "        self.init_param() \n",
    "    def build_graph(self):\n",
    "        self.extracter   = Extracter()\n",
    "        self.model_pi    = ModelPi(dim_k=self.dim_k)\n",
    "        self.model_mu    = ModelMuSigma(dim_k=self.dim_k, name='MoL Mu')\n",
    "        self.model_sigma = ModelMuSigma(dim_k=self.dim_k, name='MoL Sigma')\n",
    "    def init_param(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)): # init conv\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            if isinstance(m, nn.Linear): # init dense\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        # Heuristic: fc_mu.bias ~ Uniform(mu_min,mu_max)\n",
    "        list(self.model_mu.modules())[-1].bias.data.uniform_(self.mu_min,self.mu_max)\n",
    "    def summarize(self, x):\n",
    "        self.extracter.summarize(x)\n",
    "        with torch.no_grad():\n",
    "            f = self.extracter(x)\n",
    "        self.model_pi.summarize(f)\n",
    "        self.model_mu.summarize(f)\n",
    "        self.model_sigma.summarize(f)\n",
    "    def forward(self, x):\n",
    "        f     = self.extracter(x)\n",
    "        \n",
    "        pi    = self.model_pi(f)\n",
    "        mu    = self.model_mu(f)\n",
    "        sigma = self.sig_min + torch.exp(self.model_sigma(f))\n",
    "        return {'pi': pi, 'mu': mu, 'sigma': sigma}\n",
    "\n",
    "x = torch.randn(4, 3, 180, 320)  # demo [N x C x H x W]\n",
    "mln = MixtureLogitsNetwork()\n",
    "mln.summarize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4471,
     "status": "ok",
     "timestamp": 1610363297634,
     "user": {
      "displayName": "Sungjoon Choi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64",
      "userId": "10728677910935649939"
     },
     "user_tz": -540
    },
    "id": "cvgVHp9hcXJK",
    "outputId": "258e7a6c-eb49-495f-a5b8-66725af03bff"
   },
   "outputs": [],
   "source": [
    "def mln_uncertainties(pi,mu,sigma):\n",
    "    \"\"\"\n",
    "        :param pi:      [N x K]\n",
    "        :param mu:      [N x K x H x W]\n",
    "        :param sigma:   [N x K x H x W]\n",
    "    \"\"\"\n",
    "    # $\\pi$\n",
    "    # todo: mu_hat = torch.softmax(mu,dim=2) # logit to prob [N x K x D]\n",
    "    mu_hat = torch.sigmoid(mu)  # [N x K x H x W], 지금은 D가 없음\n",
    "    # $\\pi$\n",
    "    pi_usq = torch.unsqueeze(pi,     -1) # [N x K x 1]\n",
    "    pi_exp = torch.unsqueeze(pi_usq, -1) # [N x K x 1 x 1]\n",
    "    # softmax($\\mu$) average\n",
    "    mu_hat_avg = torch.sum(torch.mul(pi_exp, mu_hat), dim=1, keepdim=True) # [N x 1 x H x W]\n",
    "    mu_hat_diff_sq = torch.square(mu_hat - mu_hat_avg) # [N x K x H x W]\n",
    "    # Epistemic uncertainty\n",
    "    epis = torch.sum(torch.mul(pi_exp, mu_hat_diff_sq), dim=1)  # [N x H x W]\n",
    "    # todo: sum over D\n",
    "    epis = torch.sqrt(epis) # [N x H x W]\n",
    "    # Aleatoric uncertainty\n",
    "    alea = torch.sum(torch.mul(pi_exp, sigma), dim=1)  # [N x H x W]\n",
    "    # todo: sum over D\n",
    "    alea = torch.sqrt(alea) # [N x H x W]\n",
    "    # Return\n",
    "    unct_out = {'epis':epis, # [N x H x W]\n",
    "                'alea':alea  # [N x H x W]\n",
    "                }\n",
    "    return unct_out\n",
    "\n",
    "def mace_loss(pi, mu, sigma, target):\n",
    "    \"\"\"\n",
    "        :param pi:      [N x K]\n",
    "        :param mu:      [N x K x H x W]\n",
    "        :param sigma:   [N x K x H x W]\n",
    "        :param target:  [N x 1 x H x W]\n",
    "    \"\"\"\n",
    "    # $\\mu$\n",
    "    # todo: mu_hat = torch.softmax(mu,dim=2) # logit to prob [N x K x D]\n",
    "    mu_hat = torch.sigmoid(mu)  # 지금은 D가 없음\n",
    "    log_mu_hat = torch.log(mu_hat+1e-6) # [N x K x H x W]\n",
    "    # $\\pi$\n",
    "    pi_usq = torch.unsqueeze(pi,     -1) # [N x K x 1]\n",
    "    pi_exp = torch.unsqueeze(pi_usq, -1) # [N x K x 1 x 1]\n",
    "    # todo: pi는 지금 D가 없어서 expand 안 해줘도 됨\n",
    "    # CE loss\n",
    "    ce_exp = - target * log_mu_hat # CE [N x K x H x W]\n",
    "    \n",
    "    ace_exp = ce_exp / sigma  # attenuated CE [N x K x H x W]\n",
    "    mace_exp = torch.mul(pi_exp, ace_exp) # mixtured attenuated CE [N x K x H x W]\n",
    "    mace = torch.sum(mace_exp, dim=1) # [N x H x W]\n",
    "    # todo: sum over D\n",
    "    mace_avg = torch.mean(mace)\n",
    "    \n",
    "    # Compute uncertainties (epis and alea)\n",
    "    unct_out = mln_uncertainties(pi,mu,sigma)\n",
    "    epis = unct_out['epis'] # [N x H x W]\n",
    "    alea = unct_out['alea'] # [N x H x W]\n",
    "    epis_avg = torch.mean(epis)\n",
    "    alea_avg = torch.mean(alea)\n",
    "    # Return\n",
    "    loss_out = {'mace':mace, # [N x H x W]\n",
    "                'mace_avg':mace_avg, # [1]\n",
    "                'epis':epis, # [N x H x W]\n",
    "                'alea':alea, # [N x H x W]\n",
    "                'epis_avg':epis_avg, # [1]\n",
    "                'alea_avg':alea_avg # [1]\n",
    "                }\n",
    "    return loss_out\n",
    "\n",
    "def mln_gather(pi, mu, sigma):  # K 개의 expert 중 하나 선택\n",
    "    \"\"\"\n",
    "        :param pi:      [N x K]\n",
    "        :param mu:      [N x K x H x W]\n",
    "        :param sigma:   [N x K x H x W]\n",
    "    \"\"\"\n",
    "    max_idx = torch.argmax(pi, dim=1) # [N]\n",
    "    #todo: idx_gather = max_idx.unsqueeze(dim=-1).repeat(1,mu.shape[2]).unsqueeze(1) # [N x 1 x D]\n",
    "    idx_gather = max_idx.repeat(mu.shape[-2], mu.shape[-1], 1).permute(2, 0, 1).unsqueeze(dim=1)  # [N x 1 x H x W]\n",
    "    mu_sel = torch.gather(mu,dim=1,index=idx_gather).squeeze(dim=1) # [N x H x W]\n",
    "    sigma_sel = torch.gather(sigma,dim=1,index=idx_gather).squeeze(dim=1) # [N x H x W]\n",
    "    out = {'max_idx':max_idx,       # [N]\n",
    "           'idx_gather':idx_gather, # [N x 1 x H x W]\n",
    "           'mu_sel':mu_sel,         # [N x H x W]\n",
    "           'sigma_sel':sigma_sel    # [N x H x W]\n",
    "           }\n",
    "    return out\n",
    "\n",
    "def func_eval(model,data_iter,device):\n",
    "    with torch.no_grad():\n",
    "        n_total,n_correct,epis_unct_sum,alea_unct_sum = 0,0,0,0\n",
    "        model.eval() # evaluate (deactivates DropOut and BN)\n",
    "        for i, (batch_in, batch_out) in enumerate(data_iter):\n",
    "            if i == len(data_iter) // 10:\n",
    "                break\n",
    "            # Forward path\n",
    "            y_trgt = batch_out.to(device)\n",
    "            with torch.no_grad():\n",
    "                mln_out = model(batch_in.to(device))\n",
    "            pi,mu,sigma = mln_out['pi'],mln_out['mu'],mln_out['sigma']\n",
    "            out = mln_gather(pi,mu,sigma)\n",
    "            model_pred = out['mu_sel'] # [N x H x W]\n",
    "\n",
    "            # Compute uncertainty \n",
    "            unct_out = mln_uncertainties(pi,mu,sigma)\n",
    "            epis_unct = unct_out['epis'] # [N x H x W]\n",
    "            alea_unct = unct_out['alea'] # [N x H x W]\n",
    "            epis_unct_sum += torch.sum(epis_unct)\n",
    "            alea_unct_sum += torch.sum(alea_unct)\n",
    "\n",
    "            # Check predictions\n",
    "            # todo: _,y_pred = torch.max(model_pred,1)  # argmax\n",
    "            y_pred = model_pred > .8  # threshold, [N x H x W]\n",
    "            n_correct += (y_pred==y_trgt).sum().item()\n",
    "            n_total += batch_in.numel() // 3  # N x H x W\n",
    "        val_accr = (n_correct/n_total)\n",
    "        epis = (epis_unct_sum/n_total).detach().cpu().item()\n",
    "        alea = (alea_unct_sum/n_total).detach().cpu().item()\n",
    "        model.train() # back to train mode \n",
    "        out_eval = {'val_accr':val_accr, 'epis':epis, 'alea':alea}\n",
    "    return out_eval\n",
    "\n",
    "# Demo forward path of MLN\n",
    "# 걍 돌아가는지만 확인\n",
    "M = MixtureLogitsNetwork(dim_k=10).to(device)\n",
    "x = torch.randn(2, 3, 180, 320).to(device)\n",
    "target = torch.randint(2, [2, 1, 180, 320]).to(device)\n",
    "mln_out = M(x)\n",
    "pi, mu, sigma = mln_out['pi'], mln_out['mu'], mln_out['sigma']\n",
    "mu_sel = mln_gather(pi,mu,sigma)['mu_sel']\n",
    "loss_out = mace_loss(pi,mu,sigma,target)\n",
    "loss = loss_out['mace_avg'] - loss_out['epis_avg'] # epis as a regularizer \n",
    "loss.backward() # backward propagation\n",
    "print (\"x:       %s\"%(tc2np(x).shape,))\n",
    "print (\"=>\")\n",
    "print (\"pi:    %s\\n%s\"%(tc2np(pi).shape,tc2np(pi)))\n",
    "print (\"mu:    %s\\n%s\"%(tc2np(mu).shape,tc2np(mu)))\n",
    "print (\"sigma: %s\\n%s\"%(tc2np(sigma).shape,tc2np(sigma)))\n",
    "print (\"=>\")\n",
    "print (f'mace: [{loss_out[\"mace_avg\"]:.3f}]')\n",
    "# print (\"mace:[%.3f] epis:[%.3f] alea:[%.3f]\"%\n",
    "#        (loss_out['mace_avg'],loss_out['epis_avg'],loss_out['alea_avg']))\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.gcf().tight_layout() \n",
    "plt.subplot(221)\n",
    "plt.title('epis1')\n",
    "plt.imshow(tc2np(loss_out['epis'][0]))\n",
    "plt.axis('off')\n",
    "plt.subplot(222)\n",
    "plt.title('alea1')\n",
    "plt.imshow(tc2np(loss_out['alea'][0]))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title('epis2')\n",
    "plt.imshow(tc2np(loss_out['epis'][1]))\n",
    "plt.axis('off')\n",
    "plt.subplot(224)\n",
    "plt.title('alea2')\n",
    "plt.imshow(tc2np(loss_out['alea'][1]))\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9B-3N0GSmdI"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "root = Path('./data/tusimple')\n",
    "ptrain = root / 'train_set'\n",
    "pimages = root / 'train_set_preprocessed/images'\n",
    "plabels = root / 'train_set_preprocessed/labels'\n",
    "boxes = [\n",
    "    # left, top, right, bottom\n",
    "    ( 80, 45, 240, 135),  # center\n",
    "    (  0,  0, 160,  90),  # leftupper\n",
    "    (  0, 90, 160, 180),  # leftbottom\n",
    "    (160,  0, 320,  90),  # rightupper\n",
    "    (160, 90, 320, 180),  # rightbottom\n",
    "]\n",
    "class TuSimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, p_crop=.4):\n",
    "        super().__init__()\n",
    "        self.p_crop = p_crop\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "        ])\n",
    "        with open(pimages.parent / 'filenames.txt', 'r') as f:\n",
    "            self.filenames = [Path(filename) for filename in f.read().split('\\n') if filename]\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.filenames[index]\n",
    "        image = Image.open(pimages / filename.with_suffix('.jpg')).convert('RGB')\n",
    "        label = np.load(plabels / filename.with_suffix('.npy'))\n",
    "        \n",
    "        if random.random() < self.p_crop:  # crop\n",
    "            box = left, top, right, bottom = random.choice(boxes)\n",
    "            image = image.crop(box).resize((320, 180))\n",
    "            labelx = label[:,0]\n",
    "            labely = label[:,1]\n",
    "            idx = (left <= labelx) & (labelx < right) & (top  <= labely) & (labely < bottom)\n",
    "            label = label[idx]\n",
    "            label -= np.array([[left, top]])\n",
    "            label *= 2\n",
    "        \n",
    "        tmp = np.zeros((320, 180))\n",
    "        tmp[label[:,0], label[:,1]] = 1.\n",
    "        label = tmp.T[None,:,:]  # 가로세로 왜 바꾼지는 모르겠는데 암튼 바껴있음\n",
    "        return self.transform(image), torch.from_numpy(label).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "def get_train_iter(BATCH_SIZE=32):\n",
    "    tusimple_train = TuSimpleDataset()\n",
    "    return torch.utils.data.DataLoader(tusimple_train,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
    "def get_test_iter(BATCH_SIZE=1):\n",
    "    # todo: 바꿔야 됨\n",
    "    tusimple_train = TuSimpleDataset()\n",
    "    return torch.utils.data.DataLoader(tusimple_train,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, target in get_test_iter(1):\n",
    "    break\n",
    "image = image.squeeze(0).permute(1, 2, 0).numpy()\n",
    "image = np.uint8((image / 2 + .5) * 255)\n",
    "target = target.squeeze().numpy().T\n",
    "target = np.uint8(target * 255)\n",
    "\n",
    "newimage = Image.fromarray(image)\n",
    "draw = ImageDraw.Draw(newimage)\n",
    "r = 2\n",
    "for cx, cy in zip(*np.where(target==255)):\n",
    "    draw.ellipse((cx-r, cy-r, cx+r, cy+r), fill=(255,0,0,0))\n",
    "newimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFI7U7BVVvby"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18072,
     "status": "ok",
     "timestamp": 1610363311267,
     "user": {
      "displayName": "Sungjoon Choi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64",
      "userId": "10728677910935649939"
     },
     "user_tz": -540
    },
    "id": "znG-pOLgcXah",
    "outputId": "61af1ed0-c217-403f-aa06-13c2eecaeeb6"
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = get_train_iter(), get_test_iter()\n",
    "M = MixtureLogitsNetwork(dim_k=10, mu_min=-1, mu_max=+1, sig_min=0.1).to(device)\n",
    "train_accr = func_eval(M, train_iter, device)['val_accr']\n",
    "test_accr = 0\n",
    "# test_accr = func_eval(M,test_iter, device)['val_accr']\n",
    "print (\"train_accr:[%.3f] test_accr:[%.3f].\"%(train_accr,test_accr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gCwLElyKoEw"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18059,
     "status": "ok",
     "timestamp": 1610363311268,
     "user": {
      "displayName": "Sungjoon Choi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64",
      "userId": "10728677910935649939"
     },
     "user_tz": -540
    },
    "id": "N1x7PclaKopo",
    "outputId": "617de077-3d9f-4abc-8b01-e3c07c9dc2cb"
   },
   "outputs": [],
   "source": [
    "def train_wrapper(rs_rate=0.0,rp_rate=0.0,EPOCHS=10):\n",
    "    np.set_printoptions(formatter={'float_kind':'{:.2f}'.format}) \n",
    "    M = MixtureLogitsNetwork(dim_k=10, mu_min=-1, mu_max=+1, sig_min=0.1).to(device)\n",
    "    np.random.seed(seed=0)\n",
    "    torch.manual_seed(seed=0) # fix random seed\n",
    "    optm = optim.Adam(M.parameters(),lr=1e-3,weight_decay=1e-6)\n",
    "    M.train() # train mode\n",
    "    train_iter = get_train_iter(BATCH_SIZE=64)\n",
    "    test_iter = get_test_iter(BATCH_SIZE=64)\n",
    "    print_every = 1\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss_sum = 0.0\n",
    "        for batch_in, batch_out in train_iter:\n",
    "            # Forward path\n",
    "            mln_out = M(batch_in.to(device)) \n",
    "            pi, mu, sigma = mln_out['pi'], mln_out['mu'], mln_out['sigma']\n",
    "            target = batch_out.to(device) \n",
    "            loss_out = mace_loss(pi, mu, sigma, target) # 'mace_avg','epis_avg','alea_avg'\n",
    "            loss = loss_out['mace_avg'] - loss_out['epis_avg'] + loss_out['alea_avg']\n",
    "            # Update \n",
    "            optm.zero_grad() # reset gradient \n",
    "            loss.backward() # back-propagation \n",
    "            optm.step() # optimizer update\n",
    "            # Track losses \n",
    "            loss_sum += loss\n",
    "        loss_avg = loss_sum/len(train_iter)\n",
    "        # Print\n",
    "        if ((epoch%print_every)==0) or (epoch==(EPOCHS-1)):\n",
    "            train_res = func_eval(M,train_iter,device)\n",
    "            test_res  = func_eval(M,test_iter,device)\n",
    "            print (\"epoch:[%d/%d]\\n\\tloss:[%.3f] train_accr:[%.3f] test_accr:[%.3f].\"%\n",
    "                (epoch,EPOCHS,loss_avg,train_res['val_accr'],test_res['val_accr'])) \n",
    "            print (\"\\t[Train] alea:[%.3f] epis:[%.3f]\\n[Test] alea:[%.3f] epis:[%.3f]\"%\n",
    "                (train_res['alea'],train_res['epis'],test_res['alea'],test_res['epis']))\n",
    "    out = {'M':M, 'train_iter':train_iter, 'test_iter':test_iter}\n",
    "    return out \n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 249955,
     "status": "ok",
     "timestamp": 1610363543181,
     "user": {
      "displayName": "Sungjoon Choi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64",
      "userId": "10728677910935649939"
     },
     "user_tz": -540
    },
    "id": "asih03Q9_Qo8",
    "outputId": "965862bd-8f72-43f7-e086-c6f02225e789"
   },
   "outputs": [],
   "source": [
    "out = train_wrapper(EPOCHS=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, target in get_test_iter(1):\n",
    "    break\n",
    "M = out['M']\n",
    "with torch.no_grad():\n",
    "    test_out = M(image.to(device))\n",
    "mu = tc2np(torch.sigmoid(test_out['mu'].squeeze()))\n",
    "pi = tc2np(test_out['pi'])\n",
    "first, second = np.argsort(np.arange(10))[-2:][::-1]\n",
    "pred1 = mu[first]\n",
    "pred2 = mu[second]\n",
    "pi[pi < 1e-3] = 0\n",
    "print(np.squeeze(pi), np.argmax(pi))\n",
    "\n",
    "image = image.squeeze(0).permute(1, 2, 0).numpy()\n",
    "image = np.uint8((image / 2 + .5) * 255)\n",
    "target = target.squeeze().numpy().astype(np.uint8).T\n",
    "unct_out = mln_uncertainties(test_out['pi'], test_out['mu'], test_out['sigma']) \n",
    "\n",
    "newimage = Image.fromarray(image)\n",
    "draw = ImageDraw.Draw(newimage)\n",
    "r = 1\n",
    "for cx, cy in zip(*np.where(target==1)):\n",
    "    draw.ellipse((cx-r, cy-r, cx+r, cy+r), fill=(255,0,0,0))\n",
    "\n",
    "mpl.rc('font', **{'size': 15})\n",
    "mpl.rc('figure', **{'figsize': (15, 8)})\n",
    "# plt.rcParams[\"figure.figsize\"] = (14,4)\n",
    "plt.title('gt'); plt.imshow(newimage); plt.axis('off')\n",
    "plt.show()\n",
    "plt.subplot(121); plt.title('1st pred'); plt.imshow(pred1); plt.axis('off')\n",
    "plt.subplot(122); plt.title('2nd pred'); plt.imshow(pred2); plt.axis('off')\n",
    "plt.show()\n",
    "plt.subplot(121); plt.title('epis'); plt.imshow(tc2np(unct_out['epis'][0])); plt.axis('off')\n",
    "plt.subplot(122); plt.title('alea'); plt.imshow(tc2np(unct_out['alea'][0])); plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mu[8]\n",
    "plt.title('pred')\n",
    "plt.imshow(pred)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 관측\n",
    "- `epis`는 lane 부분만 작아짐 => ok\n",
    "- `BatchNorm`쓰면 2nd도 제대로 학습 안됨\n",
    "\n",
    "# 의문\n",
    "- 왜 `pred`에 규칙적인 패턴이 학습되지? => 어 이거 GAN 학습 덜 됐을 때 나오던 건데\n",
    "- 왜 `loss`에서 `epis`를 키우도록 학습시키지?\n",
    "- 왜 `loss`에서 `alea`를 키우도록 학습시키지?\n",
    "    - 이건 measurement noise가 작을 것이라 가정해서 그런 듯\n",
    "    \n",
    "# 해볼 것\n",
    "- Expert 두 개만 둬보기\n",
    "- lane을 점들이 아니라 좀 굵직굵직하게 만들기\n",
    "- 병목 왤케 심하지?\n",
    "- `epoch`가 부족한가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNeRA6+ezlSPbBd27plsloi",
   "collapsed_sections": [],
   "name": "mln.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyTorch 3.7 GPU",
   "language": "python",
   "name": "torch37gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
