{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mdn-cls.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPyhXWdJNkdMLPRxRUZXmxz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2rZ1UcDezAyc"},"source":["# MDN for Classification"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"28QgYFqVzAP7","executionInfo":{"status":"ok","timestamp":1610097769579,"user_tz":-540,"elapsed":1416,"user":{"displayName":"Sungjoon Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64","userId":"10728677910935649939"}},"outputId":"8858859b-acf0-4eae-ec11-c87a95927cac"},"source":["import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.distributions as TD\n","from torch.autograd import Variable\n","from collections import OrderedDict\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","np.set_printoptions(precision=3)\n","torch.set_printoptions(precision=3)\n","print (\"PyTorch version:[%s].\"%(torch.__version__))\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print (\"device:[%s].\"%(device))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["PyTorch version:[1.7.0+cu101].\n","device:[cuda:0].\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eQ5ZZvu4qoOv"},"source":["### Helper functions"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6o1j8QgKqqIG","executionInfo":{"status":"ok","timestamp":1610097769580,"user_tz":-540,"elapsed":1398,"user":{"displayName":"Sungjoon Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64","userId":"10728677910935649939"}},"outputId":"618f1f3f-e19f-4226-c249-18ecab60c68e"},"source":["# Codes copied from 'https://github.com/sksq96/pytorch-summary/tree/master/torchsummary' \n","def summary_string(model, input_size, batch_size=-1, device=torch.device('cuda:0'), dtypes=None):\n","    if dtypes == None:\n","        dtypes = [torch.FloatTensor]*len(input_size)\n","    summary_str = ''\n","    def register_hook(module):\n","        def hook(module, input, output):\n","            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n","            module_idx = len(summary)\n","\n","            m_key = \"%s-%i\" % (class_name, module_idx + 1)\n","            summary[m_key] = OrderedDict()\n","            summary[m_key][\"input_shape\"] = list(input[0].size())\n","            summary[m_key][\"input_shape\"][0] = batch_size\n","            if isinstance(output, (list, tuple)):\n","                summary[m_key][\"output_shape\"] = [\n","                    [-1] + list(o.size())[1:] for o in output\n","                ]\n","            else:\n","                summary[m_key][\"output_shape\"] = list(output.size())\n","                summary[m_key][\"output_shape\"][0] = batch_size\n","\n","            params = 0\n","            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n","                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n","                summary[m_key][\"trainable\"] = module.weight.requires_grad\n","            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n","                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n","            summary[m_key][\"nb_params\"] = params\n","\n","        if (\n","            not isinstance(module, nn.Sequential)\n","            and not isinstance(module, nn.ModuleList)\n","        ):\n","            hooks.append(module.register_forward_hook(hook))\n","\n","    # multiple inputs to the network\n","    if isinstance(input_size, tuple):\n","        input_size = [input_size]\n","\n","    # batch_size of 2 for batchnorm\n","    x = [torch.rand(2, *in_size).type(dtype).to(device=device)\n","         for in_size, dtype in zip(input_size, dtypes)]\n","\n","    # create properties\n","    summary = OrderedDict()\n","    hooks = []\n","\n","    # register hook\n","    model.apply(register_hook)\n","\n","    # make a forward pass\n","    # print(x.shape)\n","    model(*x)\n","\n","    # remove these hooks\n","    for h in hooks:\n","        h.remove()\n","\n","    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n","    line_new = \"{:>20}  {:>25} {:>15}\".format(\n","        \"Layer (type)\", \"Output Shape\", \"Param #\")\n","    summary_str += line_new + \"\\n\"\n","    summary_str += \"================================================================\" + \"\\n\"\n","    total_params = 0\n","    total_output = 0\n","    trainable_params = 0\n","    for layer in summary:\n","        # input_shape, output_shape, trainable, nb_params\n","        line_new = \"{:>20}  {:>25} {:>15}\".format(\n","            layer,\n","            str(summary[layer][\"output_shape\"]),\n","            \"{0:,}\".format(summary[layer][\"nb_params\"]),\n","        )\n","        total_params += summary[layer][\"nb_params\"]\n","        summary_str += line_new + \"\\n\"\n","    # return summary\n","    return summary_str,summary\n","print (\"Done.\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pm-jmaWi9Ovh"},"source":["##  $\\color{yellow}{\\text{Mixture Logits Network (MLN) }}$ \n","- \n","`Cross Entropy Loss`\n","$ \\mathcal{L}_{\\text{CE}} = \n","    -\\sum_{d=1}^{D} y_d \\log(\\hat{\\mu}_d)\n","$\n","where $y \\in [0,1]^d$ is the target and $\\hat{\\mu} \\in \\mathbb{S}^d$ is the prediction result.\n","- `Weighted CE Loss`\n","$ \\mathcal{L}_{\\text{WCE}} = \n","    -\n","    \\sum_{k=1}^{K}\n","        \\hat{\\pi}_k\n","        \\sum_{d=1}^{D} y_d \\log(\\hat{\\mu}_d)\n","$\n","where $\\hat{\\pi}$, $\\hat{\\mu}$, and $y$ are mixture weights,\n","output predicitons, and labels, respectively. \n","- \n","`Gal Loss`\n","$\n","    \\mathcal{L}_{\\text{Gal}} \n","    = \\log \\frac{1}{T} \\sum_{t}\n","        \\exp \\left(\n","            \\hat{x}_{t,c} - \\log \\sum_{c'} \\exp \\hat{x}_{t,c'}\n","            \\right)\n","$\n","where $\\hat{x_t} = f^{W} + \\sigma^{W}\\epsilon_t, ~ \\epsilon_t \\sim \\mathcal{N}(0,I)$.\n","- \n","`Mixture of Attenuated CE Loss`\n","$ \\mathcal{L}_{\\text{MACE}} \n","    =\n","    -\n","    \\sum_{k=1}^{K}\n","        \\hat{\\pi}_k\n","        \\sum_{d=1}^{D}\n","        \\frac\n","            {y_d \\log(\\hat{\\mu}_{d,k})}\n","            {\\hat{\\sigma}_{d,k} + \\sigma_{\\text{min}}}\n","$\n","where $\\sigma_{\\text{min}}=1.0$ is the minimum standard deviation.\n"]},{"cell_type":"code","metadata":{"id":"1dhslad3y48u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610097774162,"user_tz":-540,"elapsed":5963,"user":{"displayName":"Sungjoon Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64","userId":"10728677910935649939"}},"outputId":"d8188e96-61c7-4903-82b2-ba1ac39e723a"},"source":["class MixturesOfLogits(nn.Module):\n","    \"\"\"\n","        Mixture of Logits \n","    \"\"\"\n","    def __init__(self,\n","                 in_dim  = 64,  # input feature dimension\n","                 y_dim   = 10,  # output dimension\n","                 k       = 5,   # number of mixtures\n","                 sig_min = 1,   # minimum sigma\n","                 sig_max = None # maximum signa \n","                 ):\n","        super(MixturesOfLogits,self).__init__()\n","        self.in_dim   = in_dim\n","        self.y_dim    = y_dim\n","        self.k        = k\n","        self.sig_min  = sig_min\n","        self.sig_max  = sig_max\n","        self.fc_pi    = nn.Linear(self.in_dim,self.k)\n","        self.fc_mu    = nn.Linear(self.in_dim,self.k*self.y_dim)\n","        self.fc_sigma = nn.Linear(self.in_dim,self.k*self.y_dim)\n","\n","    def forward(self,x):\n","        pi_logit = self.fc_pi(x) # [N x K]\n","        pi       = torch.softmax(pi_logit,dim=1) # [N x K]\n","        mu       = self.fc_mu(x) # [N x KD]\n","        mu       = torch.reshape(mu,(-1,self.k,self.y_dim)) # [N x K x D]\n","        sigma    = self.fc_sigma(x) # [N x KD]\n","        sigma    = torch.reshape(sigma,(-1,self.k,self.y_dim)) # [N x K x D]\n","        if self.sig_max is None:\n","            sigma = self.sig_min + torch.exp(sigma) # [N x K x D]\n","        else:\n","            sigma = self.sig_min + (self.sig_max-self.sig_min)*torch.sigmoid(sigma) # [N x K x D]\n","        return pi,mu,sigma\n","\n","class MixtureLogitNetwork(nn.Module):\n","    def __init__(self,\n","                 name='mln',\n","                 x_dim   = [1,28,28], # iput dimension \n","                 k_size  = 3,         # kernel size\n","                 c_dims  = [32,64],   # channel dimensions for conv layer(s)\n","                 p_sizes = [2,2],     # pooling sizes\n","                 h_dims  = [128],     # hidden dimensions for dense layer(s)\n","                 y_dim   = 10,        # output dimension\n","                 USE_BN  = True,      # whether to use batch norm   \n","                 k       = 5,         # number of mixtures\n","                 sig_min = 1,         # $\\sigma_{min}$\n","                 sig_max = None,      # $\\sigma_{max}$\n","                 mu_min  = -3,        # minimum $\\mu$ while initializing bias \n","                 mu_max  = +3,        # maximum $\\mu$ while initializing bias \n","                 ):\n","        super(MixtureLogitNetwork,self).__init__()\n","        self.name    = name\n","        self.x_dim   = x_dim\n","        self.k_size  = k_size\n","        self.c_dims  = c_dims\n","        self.p_sizes = p_sizes\n","        self.h_dims  = h_dims\n","        self.y_dim   = y_dim\n","        self.USE_BN  = USE_BN\n","        self.k       = k\n","        self.sig_min = sig_min\n","        self.sig_max = sig_max\n","        self.mu_min  = mu_min\n","        self.mu_max  = mu_max\n","\n","        # Build graph\n","        self.build_graph()\n","\n","        # Initialize parameters        \n","        self.init_param() \n","\n","    def build_graph(self):\n","        self.layers = []\n","        # Conv layers\n","        prev_c_dim = self.x_dim[0] # input channel \n","        for (c_dim,p_size) in zip(self.c_dims,self.p_sizes):\n","            self.layers.append(\n","                nn.Conv2d(\n","                    in_channels  = prev_c_dim,\n","                    out_channels = c_dim,\n","                    kernel_size  = self.k_size,\n","                    stride       = (1,1),\n","                    padding      = self.k_size//2\n","                    ) # conv\n","                )\n","            if self.USE_BN:\n","                self.layers.append(\n","                    nn.BatchNorm2d(num_features=c_dim)\n","                )\n","            self.layers.append(nn.ReLU())\n","            self.layers.append(\n","                nn.MaxPool2d(kernel_size=(p_size,p_size),stride=(p_size,p_size))\n","                )\n","            # self.layers.append(nn.Dropout2d(p=0.1))  # p: to be zero-ed\n","            prev_c_dim = c_dim \n","        # Dense layers\n","        self.layers.append(nn.Flatten())\n","        p_prod = np.prod(self.p_sizes)\n","        prev_h_dim = prev_c_dim*(self.x_dim[1]//p_prod)*(self.x_dim[2]//p_prod)\n","        for h_dim in self.h_dims:\n","            self.layers.append(\n","                nn.Linear(\n","                    in_features  = prev_h_dim,\n","                    out_features = h_dim,\n","                    bias         = True\n","                    )\n","                )\n","            self.layers.append(nn.ReLU(True))  # activation\n","            self.layers.append(nn.Dropout2d(p=0.1))  # p: to be zero-ed\n","            prev_h_dim = h_dim\n","        # Final mixture of logits layer\n","        mol = MixturesOfLogits(\n","            in_dim  = prev_h_dim,  \n","            y_dim   = self.y_dim, \n","            k       = self.k,\n","            sig_min = self.sig_min,\n","            sig_max = self.sig_max\n","        )\n","        self.layers.append(mol)\n","\n","        # Concatanate all layers\n","        self.net = nn.Sequential()\n","        for l_idx,layer in enumerate(self.layers):\n","            layer_name = \"%s_%02d\"%(type(layer).__name__.lower(),l_idx)\n","            self.net.add_module(layer_name,layer)\n","\n","    def init_param(self): \n","        for m in self.modules():\n","            if isinstance(m,nn.Conv2d): # init conv\n","                nn.init.kaiming_normal_(m.weight)\n","                nn.init.zeros_(m.bias)\n","            if isinstance(m,nn.Linear): # lnit dense\n","                nn.init.kaiming_normal_(m.weight)\n","                nn.init.zeros_(m.bias)\n","        \"\"\"\n","        Heuristic: fc_mu.bias ~ Uniform(mu_min,mu_max)\n","        \"\"\"\n","        self.layers[-1].fc_mu.bias.data.uniform_(self.mu_min,self.mu_max)\n","\n","    def forward(self,x):\n","        return self.net(x)\n","\n","# Instantiate mixture of logits layer \n","M = MixtureLogitNetwork(\n","    name='mln',x_dim=[1,28,28],k_size=3,c_dims=[32,64],p_sizes=[2,2],\n","    h_dims=[128],y_dim=10,USE_BN=True,\n","    k=3,sig_min=1,sig_max=None,\n","    mu_min=-3,mu_max =+3).to(device)\n","print (\"Done.\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6GE5bkPJjNG3"},"source":["##  $\\color{yellow}{\\text{Loss function}}$ \n","`Mixture of Attenuated CE Loss`\n","$ \\mathcal{L}_{\\text{MACE}} \n","    =\n","    \\sum_{k=1}^{K}\n","        \\hat{\\pi}_k\n","        \\sum_{d=1}^{D}\n","        \\frac\n","            {-y_d \\log(\\hat{\\mu}_{d,k})}\n","            {\\hat{\\sigma}_{d,k} \n","            }\n","    + \n","    \\frac{1}{D}\n","    \\sum_{d=1}^{D}\n","    \\sum_{k=1}^{K}\n","    \\hat{\\pi}_k \\hat{\\sigma}_{d,k}\n","$"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZwN1fE3jMuk","executionInfo":{"status":"ok","timestamp":1610097774163,"user_tz":-540,"elapsed":5947,"user":{"displayName":"Sungjoon Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64","userId":"10728677910935649939"}},"outputId":"21df698b-e494-46c3-b96e-f85640f5a18d"},"source":["def np2tc(x_np): return torch.from_numpy(x_np).float().to(device)\n","def tc2np(x_tc): return x_tc.detach().cpu().numpy()\n","\n","def mdn_gather(pi,mu,sigma):\n","    \"\"\"\n","    pi:     [N x K]\n","    mu:     [N x K x D]\n","    sigma:  [N x K x D]\n","    \"\"\"\n","    max_idx = torch.argmax(pi,dim=1) # [N]\n","    idx_gather = max_idx.unsqueeze(dim=-1).repeat(1,mu.shape[2]).unsqueeze(1) # [N x 1 x D]\n","    mu_sel = torch.gather(mu,dim=1,index=idx_gather).squeeze(dim=1) # [N x D]\n","    sigma_sel = torch.gather(sigma,dim=1,index=idx_gather).squeeze(dim=1) # [N x D]\n","    out = {'max_idx':max_idx,'idx_gather':idx_gather,\n","           'mu_sel':mu_sel,'sigma_sel':sigma_sel}\n","    return out\n","\n","def mace_loss(pi,mu,sigma,target,alea_weight=1.0):\n","    \"\"\"\n","    Mixture of attenuated CE loss\n","        pi:      [N x K]\n","        mu:      [N x K x D]\n","        sigma:   [N x K x D]\n","        target:  [N x D]\n","    \"\"\"\n","    # softmax \\mu\n","    mu_hat = torch.softmax(mu,dim=2) # logit to prob [N x K x D]\n","    log_mu_hat = torch.log(mu_hat+1e-5) # [N x K x D]\n","    \n","    # Expanded \\pi \n","    pi_usq = torch.unsqueeze(pi,2) # [N x K x 1]\n","    pi_exp = pi_usq.expand_as(sigma) # [N x K x D]\n","\n","    # Expanded target\n","    target_usq =  torch.unsqueeze(target,1) # [N x 1 x D]\n","    target_exp =  target_usq.expand_as(sigma) # [N x K x D]\n","\n","    # Loss\n","    # ce_loss_exp = -target_exp*log_mu_hat # [N x K x D]\n","    ce_loss_exp = -target_exp*log_mu_hat # [N x K x D]\n","    atte_ce = ce_loss_exp / sigma # attenuated CE loss [N x K x D]\n","    waces = torch.sum(torch.mul(pi_exp,atte_ce),dim=1) # weighted attenuated CE loss [N x D]\n","    wace = torch.mean(waces,dim=1) # N\n","    aleas = alea_weight*torch.sum(pi_exp*sigma,dim=1)# aleatoric uncertainty [N x D]\n","    alea = torch.mean(aleas,dim=1) # [N]\n","\n","    # Accumulate loss \n","    loss = wace + alea # [N]\n","\n","    # Average loss\n","    wace_avg = torch.mean(wace) # [1]\n","    alea_avg = torch.mean(alea) # [1]\n","    loss_avg = torch.mean(loss) # [1]\n","\n","\n","    out = {'mu_hat':mu_hat,'log_mu_hat':log_mu_hat,\n","           'pi_usq':pi_usq,'pi_exp':pi_exp,\n","           'sigma':sigma,\n","           'target_usq':target_usq,'target_exp':target_exp,\n","           'ce_loss_exp':ce_loss_exp,'atte_ce':atte_ce,\n","           'waces':waces,'wace':wace,\n","           'aleas':aleas,'alea':alea,\n","           'loss':loss,\n","           'wace_avg':wace_avg,'alea_avg':alea_avg,'loss_avg':loss_avg}\n","    return out\n","\n","# Demo run to check the loss \n","M = MixtureLogitNetwork(\n","    name='mln',x_dim=[1,28,28],k_size=3,c_dims=[32,64],p_sizes=[2,2],\n","    h_dims=[128],y_dim=10,USE_BN=True).to(device)\n","\n","x_np = np.random.rand(2,1,28,28)\n","x_tc = np2tc(x_np)\n","pi_tc,mu_tc,sigma_tc = M.forward(x_tc) # forward path of MLN\n","target_tc = F.one_hot(torch.randint(low=0,high=10,size=(2,)),num_classes=10).to(device) # random one-hot\n","out = mace_loss(pi_tc,mu_tc,sigma_tc,target_tc) # mixture of CE \n","\n","print ('pi_tc:         %s'%(tc2np(target_tc).shape,))\n","print ('mu_tc:         %s'%(tc2np(mu_tc).shape,))\n","print ('sigma_tc:      %s'%(tc2np(sigma_tc).shape,))\n","print ('target_tc:     %s'%(tc2np(target_tc).shape,))\n","print ('=>')\n","print ('mu_hat:        %s'%(tc2np(out['mu_hat']).shape,))\n","print ('log_mu_hat:    %s'%(tc2np(out['log_mu_hat']).shape,))\n","print ('pi_usq:        %s'%(tc2np(out['pi_usq']).shape,))\n","print ('pi_exp:        %s'%(tc2np(out['pi_exp']).shape,))\n","print ('target_usq:    %s'%(tc2np(out['target_usq']).shape,))\n","print ('target_exp:    %s'%(tc2np(out['target_exp']).shape,))\n","print ('ce_loss_exp:   %s'%(tc2np(out['ce_loss_exp']).shape,))\n","print ('atte_ce:       %s'%(tc2np(out['atte_ce']).shape,))\n","print ('waces:         %s'%(tc2np(out['waces']).shape,))\n","print ('wace:          %s'%(tc2np(out['wace']).shape,))\n","print ('aleas:         %s'%(tc2np(out['aleas']).shape,))\n","print ('alea:          %s'%(tc2np(out['alea']).shape,))\n","print ('loss:          %s'%(tc2np(out['loss']).shape,))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["pi_tc:         (2, 10)\n","mu_tc:         (2, 5, 10)\n","sigma_tc:      (2, 5, 10)\n","target_tc:     (2, 10)\n","=>\n","mu_hat:        (2, 5, 10)\n","log_mu_hat:    (2, 5, 10)\n","pi_usq:        (2, 5, 1)\n","pi_exp:        (2, 5, 10)\n","target_usq:    (2, 1, 10)\n","target_exp:    (2, 5, 10)\n","ce_loss_exp:   (2, 5, 10)\n","atte_ce:       (2, 5, 10)\n","waces:         (2, 10)\n","wace:          (2,)\n","aleas:         (2, 10)\n","alea:          (2,)\n","loss:          (2,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JnycQ-fdqyhR"},"source":["### Summarize the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_auYKcQ6R-ui","executionInfo":{"status":"ok","timestamp":1610097774163,"user_tz":-540,"elapsed":5930,"user":{"displayName":"Sungjoon Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64","userId":"10728677910935649939"}},"outputId":"51d87bb8-df87-41cc-fccf-da778111efb6"},"source":["summary_str,summary = summary_string(M,input_size=(1,28,28),device=device)\n","print (summary_str)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 28, 28]             320\n","       BatchNorm2d-2           [-1, 32, 28, 28]              64\n","              ReLU-3           [-1, 32, 28, 28]               0\n","         MaxPool2d-4           [-1, 32, 14, 14]               0\n","            Conv2d-5           [-1, 64, 14, 14]          18,496\n","       BatchNorm2d-6           [-1, 64, 14, 14]             128\n","              ReLU-7           [-1, 64, 14, 14]               0\n","         MaxPool2d-8             [-1, 64, 7, 7]               0\n","           Flatten-9                 [-1, 3136]               0\n","           Linear-10                  [-1, 128]         401,536\n","             ReLU-11                  [-1, 128]               0\n","        Dropout2d-12                  [-1, 128]               0\n","           Linear-13                    [-1, 5]             645\n","           Linear-14                   [-1, 50]           6,450\n","           Linear-15                   [-1, 50]           6,450\n"," MixturesOfLogits-16  [[-1, 5], [-1, 5, 10], [-1, 5, 10]]               0\n","MixtureLogitNetwork-17  [[-1, 5], [-1, 5, 10], [-1, 5, 10]]               0\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R5XcNuKwO6G7"},"source":["### Check parameters"]},{"cell_type":"code","metadata":{"id":"kyDpsBwk8qr-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610097774164,"user_tz":-540,"elapsed":5915,"user":{"displayName":"Sungjoon Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64","userId":"10728677910935649939"}},"outputId":"02e985f0-f9b1-4612-b91c-f16de86affef"},"source":["n_param = 0\n","for p_idx,(param_name,param) in enumerate(M.named_parameters()):\n","    if param.requires_grad:\n","        param_numpy = param.detach().cpu().numpy() # to numpy array \n","        n_param += len(param_numpy.reshape(-1))\n","        print (\"[%02d] name:[%s] shape:[%s].\"%(p_idx,param_name,param_numpy.shape))\n","        print (\"     first 3 values:%s\"%(param_numpy.reshape(-1)[:3]))\n","print (\"Total number of parameters:[%s].\"%(format(n_param,',d')))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[00] name:[net.conv2d_00.weight] shape:[(32, 1, 3, 3)].\n","     first 3 values:[-0.002  0.223 -0.085]\n","[01] name:[net.conv2d_00.bias] shape:[(32,)].\n","     first 3 values:[0. 0. 0.]\n","[02] name:[net.batchnorm2d_01.weight] shape:[(32,)].\n","     first 3 values:[1. 1. 1.]\n","[03] name:[net.batchnorm2d_01.bias] shape:[(32,)].\n","     first 3 values:[0. 0. 0.]\n","[04] name:[net.conv2d_04.weight] shape:[(64, 32, 3, 3)].\n","     first 3 values:[-0.174 -0.067 -0.01 ]\n","[05] name:[net.conv2d_04.bias] shape:[(64,)].\n","     first 3 values:[0. 0. 0.]\n","[06] name:[net.batchnorm2d_05.weight] shape:[(64,)].\n","     first 3 values:[1. 1. 1.]\n","[07] name:[net.batchnorm2d_05.bias] shape:[(64,)].\n","     first 3 values:[0. 0. 0.]\n","[08] name:[net.linear_09.weight] shape:[(128, 3136)].\n","     first 3 values:[-0.025  0.001  0.047]\n","[09] name:[net.linear_09.bias] shape:[(128,)].\n","     first 3 values:[0. 0. 0.]\n","[10] name:[net.mixturesoflogits_12.fc_pi.weight] shape:[(5, 128)].\n","     first 3 values:[ 0.076 -0.101 -0.031]\n","[11] name:[net.mixturesoflogits_12.fc_pi.bias] shape:[(5,)].\n","     first 3 values:[0. 0. 0.]\n","[12] name:[net.mixturesoflogits_12.fc_mu.weight] shape:[(50, 128)].\n","     first 3 values:[ 0.073  0.05  -0.044]\n","[13] name:[net.mixturesoflogits_12.fc_mu.bias] shape:[(50,)].\n","     first 3 values:[0.416 2.077 0.915]\n","[14] name:[net.mixturesoflogits_12.fc_sigma.weight] shape:[(50, 128)].\n","     first 3 values:[-0.046  0.066  0.014]\n","[15] name:[net.mixturesoflogits_12.fc_sigma.bias] shape:[(50,)].\n","     first 3 values:[0. 0. 0.]\n","Total number of parameters:[434,089].\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"toW9smKjU0tH"},"source":["### Demo forward path"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1fh0_I-fPpid","executionInfo":{"status":"ok","timestamp":1610097774165,"user_tz":-540,"elapsed":5900,"user":{"displayName":"Sungjoon Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64","userId":"10728677910935649939"}},"outputId":"70cc4627-8866-4a67-f8e9-c5860a5818ef"},"source":["# Demo instantiate\n","M = MixtureLogitNetwork(\n","    name='mln',x_dim=[1,28,28],k_size=3,c_dims=[32,64],p_sizes=[2,2],\n","    h_dims=[128],y_dim=10,USE_BN=True,\n","    k=3,sig_min=1,sig_max=None,\n","    mu_min=-3,mu_max =+3).to(device)\n","# Demo forward path \n","x_np = np.random.rand(2,1,28,28)\n","x_tc = np2tc(x_np)\n","pi_tc,mu_tc,sigma_tc = M.forward(x_tc) # forward path of MLN\n","pi_np,mu_np,sigma_np = tc2np(pi_tc),tc2np(mu_tc),tc2np(sigma_tc)\n","out = mdn_gather(pi_tc,mu_tc,sigma_tc)\n","mu_sel_np = tc2np(out['mu_sel'])\n","print ('x_np:      %s'%(x_np.shape,))\n","print ('=>')\n","print ('pi_np:     %s'%(pi_np.shape,)) # [N x K]\n","print ('mu_np:     %s'%(mu_np.shape,)) # [N x K x D]\n","print ('sigma_np:  %s'%(sigma_np.shape,)) # [N x K x D]\n","print ('=>')\n","print ('mu_sel_np: %s'%(mu_sel_np.shape,)) # [N x D]"],"execution_count":7,"outputs":[{"output_type":"stream","text":["x_np:      (2, 1, 28, 28)\n","=>\n","pi_np:     (2, 3)\n","mu_np:     (2, 3, 10)\n","sigma_np:  (2, 3, 10)\n","=>\n","mu_sel_np: (2, 10)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"89CAV6pAIghW"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j1zE9HLvmi2U","executionInfo":{"status":"ok","timestamp":1610097774415,"user_tz":-540,"elapsed":6131,"user":{"displayName":"Sungjoon Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64","userId":"10728677910935649939"}},"outputId":"2948e0d7-2965-4fc7-86fd-905025dad124"},"source":["from torchvision import datasets,transforms\n","mnist_train = datasets.MNIST(root='./data/',train=True,transform=transforms.ToTensor(),download=True)\n","mnist_test = datasets.MNIST(root='./data/',train=False,transform=transforms.ToTensor(),download=True)\n","mnist_train.targets = mnist_train.targets # manipulate train labels\n","BATCH_SIZE = 64\n","train_iter = torch.utils.data.DataLoader(mnist_train,batch_size=BATCH_SIZE,shuffle=True,num_workers=1)\n","test_iter = torch.utils.data.DataLoader(mnist_test,batch_size=BATCH_SIZE,shuffle=True,num_workers=1)\n","print (\"Done.\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wYyrGiMlKIUp"},"source":["### Evaluation function"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04YxC4wEKIEy","executionInfo":{"status":"ok","timestamp":1610097774416,"user_tz":-540,"elapsed":6115,"user":{"displayName":"Sungjoon Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64","userId":"10728677910935649939"}},"outputId":"30e931d9-2e61-49ad-e8d8-4ab50bac1efb"},"source":["def func_eval(model,data_iter,device):\n","    with torch.no_grad():\n","        n_total,n_correct = 0,0\n","        model.eval() # evaluate (affects DropOut and BN)\n","        for batch_in,batch_out in data_iter:\n","            y_trgt = batch_out.to(device)\n","            pi,mu,sigma = model.forward(batch_in.view(-1,1,28,28).to(device))\n","            out = mdn_gather(pi,mu,sigma)\n","            model_pred = out['mu_sel']\n","            _,y_pred = torch.max(model_pred,1)\n","            n_correct += (y_pred==y_trgt).sum().item()\n","            n_total += batch_in.size(0)\n","        val_accr = (n_correct/n_total)\n","        model.train() # back to train mode \n","    return val_accr\n","print (\"Done\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K5luyCj0anq3","executionInfo":{"status":"ok","timestamp":1610097782913,"user_tz":-540,"elapsed":14596,"user":{"displayName":"Sungjoon Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64","userId":"10728677910935649939"}},"outputId":"74090dde-7fee-43d2-88c2-16431c91c4f7"},"source":["M.init_param()\n","train_accr = func_eval(M,train_iter,device)\n","test_accr = func_eval(M,test_iter,device)\n","print (\"train_accr:[%.3f] test_accr:[%.3f].\"%(train_accr,test_accr))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["train_accr:[0.126] test_accr:[0.124].\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vJzFw_SSJWvg"},"source":["### Train"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZS77J0Hfo8vx","executionInfo":{"status":"ok","timestamp":1610097969346,"user_tz":-540,"elapsed":201013,"user":{"displayName":"Sungjoon Choi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFkMNCaA4zshD2C87LC6X0Y7ohjLlu0sIiLepLnQ=s64","userId":"10728677910935649939"}},"outputId":"d934f0d4-4a5b-4f0b-f3b0-18e5a899b630"},"source":["np.random.seed(seed=0)\n","torch.manual_seed(seed=0)\n","M = MixtureLogitNetwork(\n","    name='mln',x_dim=[1,28,28],k_size=3,c_dims=[32,64],p_sizes=[2,2],\n","    h_dims=[128],y_dim=10,USE_BN=False,\n","    sig_min=1.0,sig_max=10,\n","    mu_min=-3,mu_max=+3).to(device)\n","M.init_param()\n","optm = optim.Adam(M.parameters(),lr=1e-3,weight_decay=1e-6)\n","M.train() # train mode\n","\n","EPOCHS,print_every = 10,1\n","for epoch in range(EPOCHS):\n","    loss_sum,wace_sum,alea_sum = 0,0,0\n","    for batch_in,batch_out in train_iter:\n","        # Forward path\n","        pi,mu,sigma = M.forward(batch_in.view(-1,1,28,28).to(device)) \n","        target = torch.eye(M.y_dim)[batch_out].to(device)\n","        mace_loss_out = mace_loss(pi,mu,sigma,target,alea_weight=0.5) # mixture of CE \n","        loss_out = mace_loss_out['loss_avg']\n","        wace_out = mace_loss_out['wace_avg']\n","        alea_out = mace_loss_out['alea_avg']\n","        # Update \n","        optm.zero_grad() # reset gradient \n","        loss_out.backward() # backpropagate\n","        optm.step() # optimizer update\n","        # Track losses \n","        loss_sum += loss_out\n","        wace_sum += wace_out\n","        alea_sum += alea_out\n","    loss_avg = loss_sum/len(train_iter)\n","    wace_avg = wace_sum/len(train_iter)\n","    alea_avg = alea_sum/len(train_iter)\n","    # Print\n","    if ((epoch%print_every)==0) or (epoch==(EPOCHS-1)):\n","        train_accr = func_eval(M,train_iter,device)\n","        test_accr = func_eval(M,test_iter,device)\n","        print (\"epoch:[%d] loss:[%.3f]=(wace:%.3f+alea:%.3f) train_accr:[%.3f] test_accr:[%.3f].\"%\n","               (epoch,loss_avg,wace_avg,alea_avg,train_accr,test_accr))\n","    \n","print (\"Done\")"],"execution_count":11,"outputs":[{"output_type":"stream","text":["epoch:[0] loss:[0.527]=(wace:0.022+alea:0.504) train_accr:[0.985] test_accr:[0.985].\n","epoch:[1] loss:[0.506]=(wace:0.006+alea:0.500) train_accr:[0.990] test_accr:[0.986].\n","epoch:[2] loss:[0.504]=(wace:0.004+alea:0.500) train_accr:[0.992] test_accr:[0.988].\n","epoch:[3] loss:[0.503]=(wace:0.003+alea:0.500) train_accr:[0.995] test_accr:[0.989].\n","epoch:[4] loss:[0.502]=(wace:0.002+alea:0.500) train_accr:[0.997] test_accr:[0.992].\n","epoch:[5] loss:[0.502]=(wace:0.002+alea:0.500) train_accr:[0.997] test_accr:[0.988].\n","epoch:[6] loss:[0.502]=(wace:0.002+alea:0.500) train_accr:[0.998] test_accr:[0.992].\n","epoch:[7] loss:[0.502]=(wace:0.002+alea:0.500) train_accr:[0.997] test_accr:[0.988].\n","epoch:[8] loss:[0.501]=(wace:0.001+alea:0.500) train_accr:[0.997] test_accr:[0.991].\n","epoch:[9] loss:[0.501]=(wace:0.001+alea:0.500) train_accr:[0.998] test_accr:[0.990].\n","Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-nAEUlM0Toe2"},"source":[""],"execution_count":null,"outputs":[]}]}